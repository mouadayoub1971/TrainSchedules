{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cree une spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Streaming from spring-boot\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kafka_df to read from kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"logsTopic\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View schema for raw kafka_df\n",
    "kafka_df.printSchema()\n",
    "#kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string into kafka_json_df\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_logs_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Define the regex pattern\n",
    "log_pattern = r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[([A-Z]+)] ([^\\s]+) ([^:]+): ([^-]+) - (.+)\"\n",
    "\n",
    "# Extract fields using the regex pattern\n",
    "parsed_kafka_logs_df = kafka_logs_df.withColumn(\"timestamp\", regexp_extract(col(\"value\"), log_pattern, 1)) \\\n",
    "    .withColumn(\"logLevel\", regexp_extract(col(\"value\"), log_pattern, 2)) \\\n",
    "    .withColumn(\"threadName\", regexp_extract(col(\"value\"), log_pattern, 3)) \\\n",
    "    .withColumn(\"loggerName\", regexp_extract(col(\"value\"), log_pattern, 4)) \\\n",
    "    .withColumn(\"message\", regexp_extract(col(\"value\"), log_pattern, 5)) \\\n",
    "    .withColumn(\"contextData\", regexp_extract(col(\"value\"), log_pattern, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema of the flattened_df, place a sample json file and change readStream to read \n",
    "parsed_kafka_logs_df.printSchema()\n",
    "#flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import   lit, to_json\n",
    "kafka_output_df = parsed_kafka_logs_df.select(\n",
    "    # Here, we are using 'threadName' as the Kafka key. You can modify this as needed.\n",
    "    lit(\"logKey\").alias(\"key\"),  # Static value for key or use a field from the data\n",
    "    to_json(col(\"value\")).alias(\"value\")  # Convert the 'value' column to a JSON string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to Kafka\n",
    "(kafka_output_df\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")  # Kafka broker address\n",
    "    .option(\"topic\", \"spark-out-put\")  # Kafka topic to send data to\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")  # Checkpoint location for fault tolerance\n",
    "    .outputMode(\"append\")  # Use append mode to add new data to the topic\n",
    "    .start()  # Start the streaming query\n",
    "    .awaitTermination()  # Keep the stream running\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
